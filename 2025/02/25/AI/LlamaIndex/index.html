
<!DOCTYPE html>
<html lang="zh_CH ">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>P-luminary || LlamaIndex</title>
    <meta name="author" content="Asuna">
    <meta name="description" content=" ">
    <meta name="keywords" content=" ">
    <link rel="icon" href="/https://raw.githubusercontent.com/P-luminary/images/master/data/asuna.jpg">
    <link rel="stylesheet" href="/css/antd.min.css">
    
    <link rel="stylesheet" href="/css/maiden-theme.css">
    
    <script src="/js/vue.js"></script>
    <script src="/js/antd.min.js"></script>
<meta name="generator" content="Hexo 6.1.0"></head>

<body>

    <div id="loading"
        style="height: 100vh; width: 100%; position: fixed;display: flex;z-index: 200; justify-content: space-between;">
        <div id="loadleft" style="width: 50%;background-color: #ffffff;transition: width 0.6s ease-out;"></div>
        <div id="loadright" style="width: 50%;background-color: #ffffff;transition: width 0.6s ease-out;"></div>
        <div
            style="position: fixed; height: 100vh; width: 100%;display: flex;justify-content: center;align-items: center;">
            <div id="loadcontent"
                style="width:400px;height:400px;padding:50px;border-radius:50%;display:flex;justify-content:center;align-items:center;border:solid 10px#a3ddfb; text-align:center;opacity:1;transition:opacity 0.3s ease-out;">
                <div>
                    <h2>LOADING...</h2>
                    <p>加载过慢请开启缓存(浏览器默认开启)</p>
                    <div>
                        <img src="/dancingkitty.gif" alt="loading">
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div id="layout">
        <transition name="into">
            <div v-show="show_page" style="display: none;">
                <div id="menu_show">
                     
<nav id="menu">
    <div class="desktop-menu">
        <a href="/">
            <span class="title">P-luminary</span>
        </a>
        
        <a href="/">
            <span>
                <a-icon type="home" theme="filled" />
            </span>
            <span>home</span>
        </a>
        
        <a href="/tags/语法">
            <span>
                <a-icon type="tags" theme="filled" />
            </span>
            <span>tags</span>
        </a>
        
    </div>

    <div :class="'phone-menu ' + menu_show" id="phone-menu">
        <div :class="'title'" @click="menu_show=!menu_show">
            <span style="margin-right: 10px;">
                <a-icon type="appstore" theme="filled" />
            </span>
            <span>P-luminary</span>
        </div>
        <div class="items" v-show="menu_show">
            
            <a href="/">
                <div class="item">
                    <div style="min-width:20px; max-width:50px; width: 10%">
                        <a-icon type="home" theme="filled" />
                    </div>
                    <div style="min-width:100px;max-width: 150%;width: 20%;">home</div>
                </div>
            </a>
            
            <a href="/tags/语法">
                <div class="item">
                    <div style="min-width:20px; max-width:50px; width: 10%">
                        <a-icon type="tags" theme="filled" />
                    </div>
                    <div style="min-width:100px;max-width: 150%;width: 20%;">tags</div>
                </div>
            </a>
            
        </div>
        <div class="curtain" v-show="menu_show"></div>
    </div>

</nav>
                </div>

                <div id="main">
                     
<link rel="stylesheet" href="/css/post-body.css">
<div class="article">
    <div>
        <h1>LlamaIndex </h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <a-icon type="calendar" theme="filled" />
            </span>
            2025/2/25
        </span>

        

        

        <span class="tags">
            <span class="icon">
                <a-icon type="tags" theme="filled" />
            </span>
            
            <span class="tag">
                
                <a href="/tags/AI" style=color:#00bcd4>
                    AI
                </a>
            </span>
            
        </span>
        
    </div>

    <div class="content" v-pre>
        <h1 id="LlamaIndex喂食给AI并进化升级"><a href="#LlamaIndex喂食给AI并进化升级" class="headerlink" title="LlamaIndex喂食给AI并进化升级"></a>LlamaIndex<del>喂食给AI并进化升级</del></h1><h4 id="将您的企业数据转化为可用于生产的LLM应用程序"><a href="#将您的企业数据转化为可用于生产的LLM应用程序" class="headerlink" title="将您的企业数据转化为可用于生产的LLM应用程序"></a>将您的企业数据转化为可用于生产的LLM应用程序</h4><p>LLM 提供人与数据之间的自然语言接口。LLM 预先训练过大量公开数据，但它们并非基于您的数据进行训练。您的数据可能是私有的，也可能是特定于您要解决的问题的数据。它隐藏在 AP1后面、SQL 数据库中，或隐藏在PDF 和幻灯片中。上下文增强使 LLM 可以使用您的数据来解决手头的问题。Llamalndex 提供构建任何上下文增强用例的工具，从原型到生产我们的工具允许您提取、解析、索引和处理您的数据，并快速实施将数据访问与 LLM 提示相结合的复杂查询工作流。<br>上下文增强最流行的示例是<strong>检索增强生成</strong>或 <strong>RAG</strong>，它在推理时将上下文与ILLM 相结合。</p>
<p>LlamaIndex 是一个将<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&zhida_source=entity">大语言模型</a>（Large Language Models, LLMs，后简称大模型）和外部数据连接在一起的工具。大模型依靠上下文学习（Context Learning）来推理知识，针对一个输入（或者是prompt），根据其输出结果。因此Prompt的质量很大程度上决定了输出结果的质量，因此提示工程（<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=Prompt+engineering&zhida_source=entity">Prompt engineering</a>）现在也很受欢迎。目前大模型的输入输出长度因模型结构、显卡算力等因素影响，都有一个长度限制（以Token为单位，ChatGPT限制长度为4k个，GPT-4是32k等，Claude最新版有个100k的）。当我们外部知识的内容超过这个长度时，就无法同时将有效的信息传递给大模型。因此就诞生了 LlamaIndex 等项目。</p>
<p>假设有一个10w的外部数据，我们的原始输入Prompt长度为100，长度限制为4k，通过查询-检索的方式，我们能将最有效的信息提取集中在这4k的长度中，与Prompt一起送给大模型，从而让大模型得到更多的信息。此外，还能通过多轮对话的方式不断提纯外部数据，达到在有限的输入长度限制下，传达更多的信息给大模型。这部分知识可参考</p>
<p>LLamaIndex的任务是通过查询、检索的方式挖掘外部数据的信息，并将其传递给大模型，因此其主要由x部分组成：</p>
<ol>
<li><p>数据连接。首先将数据能读取进来，这样才能挖掘。</p>
</li>
<li><p>索引构建。要查询外部数据，就必须先构建可以查询的索引，llamdaIndex将数据存储在Node中，并基于Node构建索引。索引类型包括<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=%E5%90%91%E9%87%8F%E7%B4%A2%E5%BC%95&zhida_source=entity">向量索引</a>、<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=%E5%88%97%E8%A1%A8%E7%B4%A2%E5%BC%95&zhida_source=entity">列表索引</a>、<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=%E6%A0%91%E5%BD%A2%E7%B4%A2%E5%BC%95&zhida_source=entity">树形索引</a>等；</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=%E6%9F%A5%E8%AF%A2%E6%8E%A5%E5%8F%A3&zhida_source=entity">查询接口</a>。有了索引，就必须提供查询索引的接口。通过这些接口用户可以与不同的 大模型进行对话，也能自定义需要的Prompt组合方式。查询接口会完成 检索+对话的功能，即先基于索引进行检索，再将检索结果和之前的输入Prompt进行（自定义）组合形成新的<strong>扩充Prompt</strong>，对话大模型并拿到结果进行解析。</p>
</li>
</ol>
<h2 id="1-数据连接器（Data-Connectors）"><a href="#1-数据连接器（Data-Connectors）" class="headerlink" title="1 数据连接器（Data Connectors）"></a><strong>1 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E8%BF%9E%E6%8E%A5%E5%99%A8&zhida_source=entity">数据连接器</a>（Data Connectors）</strong></h2><p>数据连接器，读取文档的工具，最简单的就是读取本地文件。 LLamaIndex 的数据连接器包括</p>
<ul>
<li>本地文件、Notion、Google 文档、Slack、Discord</li>
</ul>
<p>具体可参考<a href="https://link.zhihu.com/?target=https://gpt-index.readthedocs.io/en/latest/how_to/data_connectors.html">Data Connectors。</a></p>
<h2 id="2-索引结构（Index-Structures）"><a href="#2-索引结构（Index-Structures）" class="headerlink" title="2 索引结构（Index Structures）"></a><strong>2 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84&zhida_source=entity">索引结构</a>（Index Structures）</strong></h2><p>LlamaIndex 的核心其实就是 索引结构的集合，用户可以使用索引结构或基于这些索引结构自行建图。</p>
<h3 id="2-1-索引如何工作"><a href="#2-1-索引如何工作" class="headerlink" title="2.1 索引如何工作"></a><strong>2.1 索引如何工作</strong></h3><p>两个概念：</p>
<ul>
<li><strong>Node（节点）</strong>：即一段文本（Chunk of Text），LlamaIndex读取文档（documents）对象，并将其解析&#x2F;划分（parse&#x2F;chunk）成 Node 节点对象，构建起索引。</li>
<li><strong>Response Synthesis（回复合成）</strong>：LlamaIndex 进行检索节点并响应回复合成，不同的模式有不同的响应模式（比如向量查询、树形查询就不同），合成不同的扩充Prompt。</li>
</ul>
<p>索引方式包括</p>
<ul>
<li><strong>List Index</strong>：Node顺序存储，可用关键字过滤Node</li>
<li><strong>Vector Store Index</strong>：每个Node一个向量，查询的时候取top-k相似</li>
<li><strong>Tree Index</strong>：树形Node，从树根向叶子查询，可单边查询，或者双边查询合并。</li>
<li>**<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=228316377&content_type=Article&match_order=1&q=Keyword+Table+Index&zhida_source=entity">Keyword Table Index</a>**：每个Node有很多个Keywords链接，通过查Keyword能查询对应Node。</li>
</ul>
<p>不同的索引方式决定了Query选择Node方式的不同。</p>
<p>回复合成方式包括：</p>
<ul>
<li><p>创建并提纯（Create and Refine)，即线性依次迭代；</p>
</li>
<li><p>树形总结（Tree Summarize）：自底向上，两两合并，最终合并成一个回复。</p>
</li>
</ul>
<h2 id="3-查询接口（Query-Inference）"><a href="#3-查询接口（Query-Inference）" class="headerlink" title="3 查询接口（Query Inference）"></a><strong>3 查询接口（Query Inference）</strong></h2><h3 id="3-1-LlamaIndex-使用模板"><a href="#3-1-LlamaIndex-使用模板" class="headerlink" title="3.1 LlamaIndex 使用模板"></a><strong>3.1 LlamaIndex 使用模板</strong></h3><p>LlamaIndex 常用使用模版：</p>
<ol>
<li>读取文档 (手动添加or通过Loader自动添加)；</li>
<li>将文档解析为Nodes；</li>
<li>构建索引（从文档or从Nodes，如果从文档，则对应函数内部会完成第2步的Node解析）</li>
<li>[可选，进阶] 在其他索引上构建索引，即多级索引结构</li>
<li>查询索引并对话大模型</li>
</ol>
<hr>
<p><img src="https://raw.githubusercontent.com/P-luminary/images/6f518a5400d2e36ad851f0799a45911cc1bb90ff/LLM%E7%BE%8A%E9%A9%BC.png"></p>
<h6 id="LangChain-vs-LlamaIndex"><a href="#LangChain-vs-LlamaIndex" class="headerlink" title="LangChain vs LlamaIndex"></a>LangChain vs LlamaIndex</h6><p>要<strong>综合构建用于生产的高性能RAG程序</strong> 就用<code>LlamaIndex</code><br>LlamaIndex官网参考：<a target="_blank" rel="noopener" href="https://www.llamaindex.ai/">https://www.llamaindex.ai/</a><br>python文档参考：[LlamaIndex - LlamaIndex] (<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/">https://docs.llamaindex.ai/en/stable/</a>)</p>
<h6 id="LLM官网最下方的入门项目也需要学习"><a href="#LLM官网最下方的入门项目也需要学习" class="headerlink" title="LLM官网最下方的入门项目也需要学习"></a>LLM官网最下方的入门项目也需要学习</h6><h5 id="入门项目"><a href="#入门项目" class="headerlink" title="入门项目"></a>入门项目</h5><blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.npmjs.com/package/create-llama">创建骆驼</a></li>
<li><a target="_blank" rel="noopener" href="https://secinsights.ai/">SEC 洞察</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/run-llama/llamabot">骆驼机器人</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html">RAG 命令行界面</a></li>
</ul>
</blockquote>
<h4 id="RAG-Work-Flow"><a href="#RAG-Work-Flow" class="headerlink" title="RAG Work Flow"></a>RAG Work Flow</h4><p><img src="https://raw.githubusercontent.com/P-luminary/images/183f045f8c4d37aefbb62c12d2c52ebd82d26b74/RAG%20Work%20flow.png"></p>
<h4 id="构建RAG管道—加载数据-镊取"><a href="#构建RAG管道—加载数据-镊取" class="headerlink" title="构建RAG管道—加载数据(镊取)"></a>构建RAG管道—加载数据(镊取)</h4><p>在您选择的 LLM 可以处理您的数据之前，您首先需要处理数据并加载数据。这与 ML 领域的数据清理&#x2F;特征工程管道或传统数据设置中的 ETL 管道有相似之处。</p>
<p>此引入管道通常包括三个主要阶段：</p>
<ol>
<li>加载数据</li>
<li>转换数据</li>
<li>索引和存储数据</li>
</ol>
<p>我们将在<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/">后面</a><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/understanding/storing/storing/">的章节</a>中介绍索引 &#x2F; 存储。在本指南中，我们将主要讨论 loader 和 transformations。</p>
<h4 id="装载机"><a href="#装载机" class="headerlink" title="装载机"></a>装载机</h4><p>在您选择的 LLM 可以处理您的数据之前，您需要加载它。LlamaIndex 执行此作的方式是通过数据连接器（也称为 .Data Connector 从不同的数据源摄取数据并将数据格式化为对象。A 是有关该数据的数据（<u>当前为<strong>文本</strong>，将来为<strong>图像</strong>和<strong>音频</strong></u>）和元数据的集合。<code>Reader Document Document</code></p>
<h4 id="使用-SimpleDirectoryReader-加载"><a href="#使用-SimpleDirectoryReader-加载" class="headerlink" title="使用 SimpleDirectoryReader 加载"></a>使用 SimpleDirectoryReader 加载</h4><p>最容易使用的阅读器是我们的 SimpleDirectoryReader，它从给定目录中的每个文件创建文档。它内置于 LlamaIndex 中，可以读取多种格式，包括 <strong>Markdown、PDF、Word 文档、PowerPoint 幻灯片、图像、音频和视频</strong>。</p>
<pre><code class="python">from llama_index.core import SimpleDirectoryReader
documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
</code></pre>
<p>★ ★ ★ ★ <u>更多教程请看<strong>官方接口文档</strong></u> <a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/understanding/loading/loading/"><span style = "color:blue"><strong><u>Loading Data (Ingestion) - LlamaIndex</u></strong></span></a> ★ ★ ★ ★</p>
<p><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/getting_started/starter_example/">Starter Tutorial - LlamaIndex</a> + <a target="_blank" rel="noopener" href="https://colab.research.google.com/#scrollTo=S7b85AZVL5M5">欢迎使用 Colaboratory - Colab</a> &#x3D; 在线使用代码</p>
<p><img src="https://raw.githubusercontent.com/P-luminary/images/640d982cf931112877ec183a26a25a678da433be/LlamaIndex+Colab.png"></p>
<h4 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h4><p>RAG，也称为检索增强生成，是利用个人或私域数据增强 <code>LLM</code> 的一种范式。通常，它包含两个阶段：</p>
<ol>
<li><p>索引</p>
<p>构建知识库。</p>
</li>
<li><p>查询</p>
<p>从知识库检索相关上下文信息，以辅助 <code>LLM</code> 回答问题。</p>
</li>
</ol>
<p><code>LlamaIndex</code> 提供了工具包帮助开发者极其便捷地完成这两个阶段的工作。</p>
<h4 id="索引阶段"><a href="#索引阶段" class="headerlink" title="索引阶段"></a>索引阶段</h4><p><code>LlamaIndex</code> 通过提供 Data connectors(数据连接器) 和 Indexes (索引) 帮助开发者构建知识库。</p>
<p>该阶段会用到如下工具或组件：</p>
<ul>
<li><p>Data connectors</p>
<p>数据连接器。它负责将来自不同数据源的不同格式的数据注入，并转换为 <code>LlamaIndex</code> 支持的文档（Document）表现形式，其中包含了文本和元数据。</p>
</li>
<li><p>Documents &#x2F; Nodes</p>
<p>Document是 <code>LlamaIndex</code> 中容器的概念，它可以包含任何数据源，包括，PDF文档，API响应，或来自数据库的数据。</p>
<p>Node是 <code>LlamaIndex</code> 中数据的最小单元，代表了一个 Document的分块。它还包含了元数据，以及与其他Node的关系信息。这使得更精确的检索操作成为可能。</p>
</li>
<li><p>Data Indexes</p>
<p><code>LlamaIndex</code> 提供便利的工具，帮助开发者为注入的数据建立索引，使得未来的检索简单而高效。</p>
<p>最常用的索引是向量存储索引 - <code>VectorStoreIndex</code>。</p>
</li>
</ul>
<h4 id="查询阶段"><a href="#查询阶段" class="headerlink" title="查询阶段"></a>查询阶段</h4><p>在查询阶段，<code>RAG</code> 管道根据的用户查询，检索最相关的上下文，并将其与查询一起，传递给 <code>LLM</code>，以合成响应。这使 <code>LLM</code> 能够获得不在其原始训练数据中的最新知识，同时也减少了虚构内容。该阶段的关键挑战在于检索、编排和基于知识库的推理。</p>
<p><code>LlamaIndex</code> 提供可组合的模块，帮助开发者构建和集成 <code>RAG</code> 管道，用于问答、聊天机器人或作为代理的一部分。这些构建块可以根据排名偏好进行定制，并组合起来，以结构化的方式基于多个知识库进行推理。</p>
<p>该阶段的构建块包括：</p>
<ul>
<li><p><strong>Retrievers</strong></p>
<p>检索器。它定义如何高效地从知识库，基于查询，检索相关上下文信息。</p>
</li>
<li><p><strong>Node Postprocessors</strong></p>
<p>Node后处理器。它对一系列文档节点（Node）实施转换，过滤，或排名。</p>
</li>
<li><p><strong>Response Synthesizers</strong></p>
<p>响应合成器。它基于用户的查询，和一组检索到的文本块（形成上下文），利用 <code>LLM</code> 生成响应。</p>
</li>
</ul>
<p>RAG管道包括：</p>
<ul>
<li><p><strong>Query Engines</strong></p>
<p>查询引擎 - 端到端的管道，允许用户基于知识库，以自然语言提问，并获得回答，以及相关的上下文。</p>
</li>
<li><p><strong>Chat Engines</strong></p>
<p>聊天引擎 - 端到端的管道，允许用户基于知识库进行对话（多次交互，会话历史）。</p>
</li>
<li><p><strong>Agents</strong></p>
<p>代理。它是一种由 <code>LLM</code> 驱动的自动化决策器。代理可以像查询引擎或聊天引擎一样使用。主要区别在于，代理动态地决定最佳的动作序列，而不是遵循预定的逻辑。这为其提供了处理更复杂任务的额外灵活性。</p>
</li>
</ul>
<h4 id="LlamaIndex个性化配置"><a href="#LlamaIndex个性化配置" class="headerlink" title="LlamaIndex个性化配置"></a>LlamaIndex个性化配置</h4><p><code>LlamaIndex</code> 对 <code>RAG</code> 过程提供了全面的配置支持，允许开发者对整个过程进行个性化设置。常见的配置场景包括：</p>
<ul>
<li>自定义文档分块</li>
<li>自定义向量存储</li>
<li>自定义检索</li>
<li>指定 <code>LLM</code></li>
<li>指定响应模式</li>
<li>指定流式响应</li>
</ul>
<p>注，个性化配置主要通过 <code>LlamaIndex</code> 提供的 <code>ServiceContext</code> 类实现。</p>
<h5 id="配置场景示例"><a href="#配置场景示例" class="headerlink" title="配置场景示例"></a>配置场景示例</h5><p>接下来通过简明示例代码段展示 <code>LlamaIndex</code> 对各种配置场景的支持。</p>
<h5 id="自定义文档分块"><a href="#自定义文档分块" class="headerlink" title="自定义文档分块"></a>自定义文档分块</h5><pre><code class="python">from llama_index import ServiceContext
service_context = ServiceContext.from_defaults(chunk_size=500)
</code></pre>
<h5 id="自定义向量存储"><a href="#自定义向量存储" class="headerlink" title="自定义向量存储"></a>自定义向量存储</h5><pre><code class="python">import chromadb
from llama_index.vector_stores import ChromaVectorStore
from llama_index import StorageContext

chroma_client = chromadb.PersistentClient()
chroma_collection = chroma_client.create_collection(&quot;quickstart&quot;)
# 向量存储
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
</code></pre>
<h5 id="自定义检索"><a href="#自定义检索" class="headerlink" title="自定义检索"></a>自定义检索</h5><p>自定义检索中，我们可以通过参数指定<strong>查询引擎</strong>(Query Engine)在检索时请求的相似文档数。</p>
<pre><code class="python">index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine(similarity_top_k=5)
</code></pre>
<h5 id="指定-LLM"><a href="#指定-LLM" class="headerlink" title="指定 LLM"></a>指定 <code>LLM</code></h5><pre><code class="python"># 指定大语言模型
service_context = ServiceContext.from_defaults(llm=OpenAI())
</code></pre>
<h5 id="指定响应模式"><a href="#指定响应模式" class="headerlink" title="指定响应模式"></a>指定响应模式</h5><pre><code class="python">query_engine = index.as_query_engine(response_mode=&#39;tree_summarize&#39;)
</code></pre>
<h5 id="指定流式响应"><a href="#指定流式响应" class="headerlink" title="指定流式响应"></a>指定流式响应</h5><pre><code class="python"># 流式响应
query_engine = index.as_query_engine(streaming=True)
</code></pre>
<h5 id="完整实例"><a href="#完整实例" class="headerlink" title="完整实例"></a>完整实例</h5><p><strong>GitHub</strong>：<a target="_blank" rel="noopener" href="https://github.com/sugarforever/LlamaIndex-Tutorials/blob/main/03_Customization/03_Customization.ipynb">LlamaIndex-Tutorials&#x2F;03_Customization&#x2F;03_Customization.ipynb at main · sugarforever&#x2F;LlamaIndex-Tutorials</a><br><strong>Colab</strong>：<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/sugarforever/LlamaIndex-Tutorials/blob/main/03_Customization/03_Customization.ipynb">03_Customization.ipynb - Colab</a></p>
<p>请参考<a target="_blank" rel="noopener" href="https://github.com/sugarforever/LlamaIndex-Tutorials/blob/main/03_Customization/03_Customization.ipynb">03_Customization.ipynb</a> ，这是一个基于第1课的示例实现上述的所有个性化配置：</p>
<ol>
<li>文档分块大小：500</li>
<li>Chromadb作为向量存储</li>
<li>自定义检索文档数为5</li>
<li>指定大模型为OpenAI的模型</li>
<li>响应模式为 <code>tree_summarize</code></li>
<li>问答实现流式响应</li>
</ol>
<p>注，响应模式会在后续课程中详细介绍。</p>
<hr>
<h4 id="强大的数据连接器"><a href="#强大的数据连接器" class="headerlink" title="强大的数据连接器"></a>强大的数据连接器</h4><p><strong>开源教程</strong>：<a target="_blank" rel="noopener" href="https://github.com/sugarforever/LlamaIndex-Tutorials/tree/main/04_Data_Connectors">LlamaIndex-Tutorials&#x2F;04_Data_Connectors at main · sugarforever&#x2F;LlamaIndex-Tutorials</a><br><strong>演示实例</strong>：<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/sugarforever/LlamaIndex-Tutorials/blob/main/04_Data_Connectors/04_Data_Connectors.ipynb">04_Data_Connectors.ipynb - Colab</a></p>
<h4 id="文档与节点"><a href="#文档与节点" class="headerlink" title="文档与节点"></a>文档与节点</h4><p><strong>开源教程</strong>：<a target="_blank" rel="noopener" href="https://github.com/sugarforever/LlamaIndex-Tutorials/tree/main/05_Documents_Nodes">LlamaIndex-Tutorials&#x2F;05_Documents_Nodes at main · sugarforever&#x2F;LlamaIndex-Tutorials</a></p>
<h4 id="LlamaPack-新手入门"><a href="#LlamaPack-新手入门" class="headerlink" title="LlamaPack 新手入门"></a>LlamaPack 新手入门</h4><p><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/community/llama_packs/">LlamaPacks - LlamaIndex</a></p>

    </div>

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <div id="comment">
        <div id="gitalk-container">
        </div>
    </div>
    
</div>
                     
<footer id="footer">
    <div class="footer-wrap">
        <div>
            © 2018 - 2025 P-luminary
            <span class="footer-icon">
                <a-icon type="flag" theme="filled" /></span>
            @Asuna
        </div>
        <div></div>
        <div>Based on the <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo Engine</a>
<!--         & <a target="_blank" rel="noopener" href="https://github.com/korilin/hexo-theme-particle">Particle Theme</a></div> -->
        
    </div>

</footer>

<script src="/js/highlight.min.js"></script>
<script src="/js/particle.js"></script>
                </div>
            </div>
        </transition>
    </div>

    <script>
    new Vue({
        el: "#layout",
        data: {
            show_page: false,
            onload_menu: false,
            menu_show: false,
            card_top: 100
        },
        created: function () {
            var that = this
            window.onload = function () {
                that.show_page = true
                document.getElementById("loadcontent").style.opacity = 0
                setTimeout(function () {
                    document.getElementById("loadleft").style.width = 0
                    document.getElementById("loadright").style.width = 0
                }, 300)
                setTimeout(function () {
                    document.getElementById("loading").style = "display:none"
                }, 600)
            }
        },
        mounted: function () {
            var that = this
            window.addEventListener('scroll', function (e) {
                that.menu_show = false
            })
        },
        methods: {
            home_click: function () {
                window.scrollTo({
                    top: window.innerHeight - 80,
                    behavior: "smooth",
                });
            }
        }
    })
</script>

<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script>
    const gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: 'hexo-theme-particle',      // The repository of store comments,
        owner: 'P-luminary',
        admin: ['P-luminary'],
        language: 'en',
        id: location.pathname,      // Ensure uniqueness and length less than 50
        distractionFreeMode: true  // Facebook-like distraction free mode
    })
    gitalk.render('gitalk-container')
</script>

</body>

</html>